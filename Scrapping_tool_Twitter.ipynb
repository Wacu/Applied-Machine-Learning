{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wacu/Applied-Machine-Learning/blob/main/Scrapping_tool_Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5087684e",
      "metadata": {
        "id": "5087684e"
      },
      "outputs": [],
      "source": [
        "# pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84cf9de",
      "metadata": {
        "id": "c84cf9de"
      },
      "outputs": [],
      "source": [
        "import snscrape.modules.twitter as twitter\n",
        "maxTweets = 10\n",
        "keyword='tb2'\n",
        "for i, tweet in enumerate(twitter.TwitterSearchScraper(keyword + ' since:2021-11-01 until:2023-01-01 lang:\"en\" ').get_items()):\n",
        "    tweets = {\n",
        "            \"Tweet\" : \"Reply\",\n",
        "            \"tweet/reply id\": \"a\"+str(tweet.id),\n",
        "            \"inReplyToTweetId\": \"a\"+str(tweet.inReplyToTweetId),\n",
        "            \"conversationId\": \"a\"+str(tweet.conversationId),\n",
        "            \"tweet.username\" : tweet.username,\n",
        "            \"tweet.content\" : tweet.content,\n",
        "            \"tweet.date\" : tweet.date,\n",
        "            \"tweet.user.location\" : tweet.user.location,\n",
        "            \"tweet.likeCount\" : tweet.likeCount, \n",
        "            \"tweet.replyCount\" : tweet.replyCount,\n",
        "            \"tweet.retweetCount\" : tweet.retweetCount,\n",
        "            \"tweet.user.followersCount\" : tweet.user.followersCount,\n",
        "            \"tweet.user.description\": tweet.user.description,\n",
        "            \"tweet.user.friendsCount\": tweet.user.friendsCount,\n",
        "            \"tweet.user.statusesCount\": tweet.user.statusesCount,\n",
        "            \"tweet.user.favouritesCount\": tweet.user.favouritesCount,\n",
        "            \"tweet.user.listedCount\": tweet.user.listedCount,\n",
        "            \"tweet.user.mediaCount\": tweet.user.mediaCount,\n",
        "            \"tweet.url\" : tweet.url\n",
        "            }   \n",
        "#     print(tweets) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c7e120",
      "metadata": {
        "scrolled": false,
        "id": "17c7e120"
      },
      "outputs": [],
      "source": [
        "# Scrape 100 tweets containing the words python and learning since 2020–11–01 until 2021–01–01.\n",
        "\n",
        "\n",
        "import snscrape.modules.twitter as twitter\n",
        "\n",
        "maxTweets = 100\n",
        "def exactPhrase(keyword=''):\n",
        "    for i, tweet in enumerate(twitter.TwitterSearchScraper(keyword + ' since:2020-11-01 until:2021-01-01 ').get_items()):\n",
        "        if i > maxTweets:\n",
        "            break\n",
        "        print(i)\n",
        "        print(tweet.username)\n",
        "        print(tweet.content)\n",
        "        print(tweet.url)\n",
        "        print('\\n')\n",
        "\n",
        "twit= exactPhrase('\"Davis&shirtliff\"')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1d5fa3d",
      "metadata": {
        "id": "c1d5fa3d"
      },
      "source": [
        "### Scrape tweets sent from user to another user\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e02735",
      "metadata": {
        "id": "c7e02735",
        "outputId": "7bcb78ff-0c3c-4502-e219-6571f86c4549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Projects_007\n",
            "2020-10-31 20:27:28+00:00\n",
            "@mertcobanov Hocam artık bi proje baslatirsiniz :)\n",
            "https://twitter.com/Projects_007/status/1322636326597001217\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# crape sent tweets from Projects_007 user to mertcobanov user only 100 Turkish tweets since 2020–01–01 until 2020–12–30.\n",
        "\n",
        "\n",
        "import snscrape.modules.twitter as twitter\n",
        "\n",
        "maxTweets = 5\n",
        "for i,tweet in enumerate(twitter.TwitterSearchScraper('from:Projects_007  to:@mertcobanov lang:tr since:2020-01-01 until:2020-12-30').get_items()):\n",
        "        if i > 1 : break  \n",
        "        print(tweet.user.username)\n",
        "        print(tweet.date)\n",
        "        print(tweet.rawContent)\n",
        "        print(tweet.url)\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6741c15",
      "metadata": {
        "id": "d6741c15"
      },
      "source": [
        "### Scrape Tweets Sent from a Certain Region\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8adea1c",
      "metadata": {
        "id": "c8adea1c",
        "outputId": "946251ee-2874-4fee-fcaf-fadd146d2a5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "studio_daddy\n",
            "2020-12-29 23:59:26+00:00\n",
            "@justin_loans I wish\n",
            "\n",
            "\n",
            "dj_sir\n",
            "2020-12-29 23:59:17+00:00\n",
            "@Madonna @Queen_UK @Charles_HRH @TeamMeghanM @PHarry_Meghan @TheEllenShow @mrdannyglover @OutpostStudios @WilkesBashford @armani @StevieNicks @MickFleetwood @Xfinity @Facebook @Google @MetroByTMobile @JuanaPrincess2 @taylorswift13 @VirtualDJ @george_clinton @ThomasAndersGoM @LionelRichie @MileyCyrus @katyperry @LGUSAMobile @DJMAGICD1 @DDjmagidelic @AmericanIdol @MadonnaIllumin1 @Madonnaillunina @DollyParton @YouTube @shrinershosp @shriners @dollyslibrary @PrincePhilipDoE @itstonybennett @Nickslive @fleetwoodmac @ellentube @Android Number one everybody's posting goodbye wall but they won't allow me to post on my on my official wall @Facebook knowing god damn well no one's going to know where I'm at if I can't that automatically tells you they're shady\n",
            "\n",
            "\n",
            "NorCalStoolie\n",
            "2020-12-29 23:58:50+00:00\n",
            "Does @PGATOUR2K freeze for anyone else in between shots during a tournament?? #pga2k1\n",
            "\n",
            "\n",
            "lucyinthasky___\n",
            "2020-12-29 23:57:46+00:00\n",
            "@ViviSolisok Same, yo me tiro al pasto y me broto toda. Termino volviendo en ambulancia BUE\n",
            "\n",
            "\n",
            "mistyb415\n",
            "2020-12-29 23:57:19+00:00\n",
            "I'm at Jose Ortega Elementary School in San Francisco, CA https://t.co/i73RpPTJGk\n",
            "\n",
            "\n",
            "southbaydev\n",
            "2020-12-29 23:57:03+00:00\n",
            "“Let the beauty of what you love be what you do” - Rumi\n",
            ".\n",
            "A message we can all relate to after 2020. https://t.co/maIPS8gx5Y\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import snscrape.modules.twitter as twitter\n",
        "\n",
        "maxTweets = 5\n",
        "for i,tweet in enumerate(twitter.TwitterSearchScraper(' geocode:37.7764685,-122.4172004,10km since:2020-01-01 until:2020-12-30').get_items()):\n",
        "        if i > maxTweets : break  \n",
        "        print(tweet.user.username)\n",
        "        print(tweet.date)\n",
        "        print(tweet.rawContent)\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb56e0b4",
      "metadata": {
        "id": "fb56e0b4"
      },
      "source": [
        "### Download Images and Videos from Tweets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d3717c",
      "metadata": {
        "id": "e0d3717c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import requests\n",
        "import urllib.request\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "class Video_Image():\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def photo_save(self, photo_url):\n",
        "        '''Save photo from url'''\n",
        "        a = urlparse(photo_url)\n",
        "        photo_name = os.path.basename(a.path)\n",
        "        with open(photo_name+'.jpg', 'wb') as f:\n",
        "            f.write(requests.get(photo_url).content)\n",
        "\n",
        "    def video_save(self, url_link):\n",
        "        '''Save video from url'''\n",
        "        a = urlparse(url_link)\n",
        "        video_name = os.path.basename(a.path)\n",
        "        urllib.request.urlretrieve(url_link, video_name+'.mp4')\n",
        "        \n",
        "    def get_video_image(self):\n",
        "        ''' Get User's tweets videos and images url'''\n",
        "        username ='Davis & Shirtliff'\n",
        "        # username=\"Sinamekiserdar\"\n",
        "        for i, tweet in enumerate(sntwitter.TwitterSearchScraper('from:' +username+ ' since:2022-08-01 until:2023-01-01 ').get_items()):\n",
        "            if tweet.media:\n",
        "                for medium in tweet.media:\n",
        "                    if medium.__class__.__name__ == 'Photo':\n",
        "                        self.photo_save(medium.fullUrl)\n",
        "                    elif medium.__class__.__name__ == 'Video':\n",
        "                        for v in medium.variants:\n",
        "                            if '.mp4' in v.url : self.video_save(v.url)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    cls = Video_Image()\n",
        "    cls.get_video_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "617fb2df",
      "metadata": {
        "id": "617fb2df"
      },
      "source": [
        "### Scrape Conversation Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb7d6bd",
      "metadata": {
        "id": "6bb7d6bd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import snscrape.modules.twitter as sntwitter\n",
        "# for i,tweet in enumerate(sntwitter.TwitterSearchScraper(' conversation_id:1425083028695658504 (filter:safe OR -filter:safe) ')\\\n",
        "#                          .get_items()):\n",
        "#     print(i, tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88ab81c8",
      "metadata": {
        "id": "88ab81c8"
      },
      "source": [
        "### Scrape Tweets to Csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92d42f45",
      "metadata": {
        "id": "92d42f45"
      },
      "outputs": [],
      "source": [
        "\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import csv\n",
        "\n",
        "# keyword = 'Python'\n",
        "username = \"AnyembeL\"\n",
        "\n",
        "with open('Projects_007.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper('from:' +username+ ' since:2020-11-01 until:2021-01-01 lang:\"en\" ').get_items()):\n",
        "        writer.writerow([tweet.username, tweet.content.encode(\"utf-8\"), \n",
        "                         tweet.date, tweet.user.location,tweet.likeCount, \n",
        "                         tweet.retweetCount, tweet.user.followersCount, tweet.url])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4171de02",
      "metadata": {
        "id": "4171de02"
      },
      "outputs": [],
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas\n",
        "\n",
        "# Creating list to append tweet data to\n",
        "tweets_list2 = []\n",
        "\n",
        "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('COVID Vaccine since:2021-01-01 until:2021-05-31').get_items()):\n",
        "    if i>5000:\n",
        "        break\n",
        "    tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
        "    \n",
        "# Creating a dataframe from the tweets list above\n",
        "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1730827",
      "metadata": {
        "id": "b1730827"
      },
      "source": [
        "### Scraping a specific Twitter user’s tweets and store in pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7589c318",
      "metadata": {
        "id": "7589c318"
      },
      "outputs": [],
      "source": [
        "\n",
        "# importing libraries and packages\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "\n",
        "# Creating list to append tweet data \n",
        "\n",
        "def scrapp_twitter_by_user(num,username=''):\n",
        "    tweets_list1 = []\n",
        "    \n",
        "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper\\\n",
        "                             ('from:' +username+ ' since:2010-01-01 until:2022-1-30 lang:\"en\" ')\\\n",
        "                             .get_items()): #declare a username \n",
        "        if i>num: #number of tweets you want to scrape\n",
        "            break\n",
        "        tweets_list1.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username]) #declare the attributes to be returned\n",
        "\n",
        "# Creating a dataframe from the tweets list above \n",
        "    tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
        "    return tweets_df1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d879c11",
      "metadata": {
        "id": "4d879c11"
      },
      "outputs": [],
      "source": [
        "data=scrapp_twitter_by_user(200,'Davis&Shirtliff ')\n",
        "data.to_csv('sam_tweets.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af83229",
      "metadata": {
        "id": "5af83229"
      },
      "source": [
        "### Scraping tweets from a text search query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33ba0f90",
      "metadata": {
        "id": "33ba0f90"
      },
      "outputs": [],
      "source": [
        "\n",
        "# importing libraries and packages\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "\n",
        "# Creating list to append tweet data \n",
        "\n",
        "def scrapp_twitter_by_search_query(num,keyword=''):\n",
        "    tweets_list1 = []\n",
        "    \n",
        "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper\\\n",
        "                             (keyword+ ' since:2010-01-01 until:2022-11-30 lang:\"en\" ')\\\n",
        "                             .get_items()): #declare a username \n",
        "        if i>num: #number of tweets you want to scrape\n",
        "            break\n",
        "        tweets_list1.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username,tweet.user.location,\\\n",
        "                            tweet.likeCount]) #declare the attributes to be returned\n",
        "\n",
        "# Creating a dataframe from the tweets list above \n",
        "    tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Tweet Id', 'Text', 'Username','location','likes'])\n",
        "    return tweets_df1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc0ee2a8",
      "metadata": {
        "id": "cc0ee2a8",
        "outputId": "aaa2d654-0e56-48d8-8e89-3cd0aa8ceca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jamhuri\n"
          ]
        }
      ],
      "source": [
        "# Get the twitter for the last\n",
        "search_list=['william ruto','raila odinga','Rigathi gachagua', 'riggy G','Kalonzo musyoka', 'martha karua']\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "search_list=['Jamhuri']\n",
        "\n",
        "for i in range(0,len(search_list)):\n",
        "    print(search_list[i])\n",
        "    data=scrapp_twitter_by_search_query(10,search_list[i])\n",
        "    data.to_csv(f'{search_list[i]}.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "817e290f",
      "metadata": {
        "id": "817e290f"
      },
      "source": [
        "### A simple code but very effective by Location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ddd8fe2",
      "metadata": {
        "id": "9ddd8fe2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ed6f78e",
      "metadata": {
        "id": "4ed6f78e"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "loc = '-1.27467,36.81178, 100km'\n",
        "df_coord = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper\\\n",
        "                                         ('kawira mwangaza geocode:\"{}\"'.format(loc)).get_items(),6000))[['user', 'date','rawContent',\\\n",
        "                                                                                            'likeCount','retweetCount']]\n",
        "\n",
        "df_coord['user_location'] =  df_coord['user'].apply(lambda x: x['location'])\n",
        "\n",
        "\n",
        "df_coord.to_csv('KM.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f0ee177",
      "metadata": {
        "id": "6f0ee177"
      },
      "outputs": [],
      "source": [
        "df_coord.user[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fedb0f35",
      "metadata": {
        "id": "fedb0f35",
        "outputId": "47ebc4ea-d8f9-4d92-9a9f-68d51811efd5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>date</th>\n",
              "      <th>rawContent</th>\n",
              "      <th>likeCount</th>\n",
              "      <th>retweetCount</th>\n",
              "      <th>user_location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'username': 'kenyagistcom', 'id': 10094307210...</td>\n",
              "      <td>2022-12-07 23:49:11+00:00</td>\n",
              "      <td>How Raila Pleaded With Chebukati Before Declar...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Nairobi, Kenya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'username': 'EufrTosh', 'id': 1454564682, 'di...</td>\n",
              "      <td>2022-12-07 23:40:01+00:00</td>\n",
              "      <td>@MigunaMiguna @RailaOdinga Unapenda Raila na r...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Nairobi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                user  \\\n",
              "0  {'username': 'kenyagistcom', 'id': 10094307210...   \n",
              "1  {'username': 'EufrTosh', 'id': 1454564682, 'di...   \n",
              "\n",
              "                       date  \\\n",
              "0 2022-12-07 23:49:11+00:00   \n",
              "1 2022-12-07 23:40:01+00:00   \n",
              "\n",
              "                                          rawContent  likeCount  retweetCount  \\\n",
              "0  How Raila Pleaded With Chebukati Before Declar...          0             0   \n",
              "1  @MigunaMiguna @RailaOdinga Unapenda Raila na r...          0             0   \n",
              "\n",
              "    user_location  \n",
              "0  Nairobi, Kenya  \n",
              "1         Nairobi  "
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_coord"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "984c8bc7",
      "metadata": {
        "id": "984c8bc7"
      },
      "source": [
        "https://blog.devgenius.io/twitter-sentiment-analysis-hate-speech-detection-with-unbalanced-data-46d1f57be2ab"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}